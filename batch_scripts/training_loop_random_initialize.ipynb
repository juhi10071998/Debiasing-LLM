{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec63901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from soft_embedding import SoftEmbedding\n",
    "from dataset import BiasDataset\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51f128f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79efb7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO CHECK WHETHER THE output_1.logits[:, -1, :] returns the next word logits for each sentence in the batch.\n",
    "\n",
    "def get_next_word_log_probabilities(model_input_1, model_input_2, model, logsoftmax):\n",
    "    #import pdb; pdb.set_trace()\n",
    "    output_1 = model(input_ids = model_input_1[0],attention_mask = model_input_1[1])     \n",
    "    #here we get the logits from the output, the logits are obtained from the output.logits, here we first get\n",
    "    #the [batch_size, sequence_len, vocab_size]\n",
    "    #next_word_logits_1 = torch.stack([torch.index_select(seq_logits, 0, torch.tensor([i+1])) for i, seq_logits in enumerate(output_1.logits)])\n",
    "    next_word_logits_1 = output_1.logits[:, -1, :]\n",
    "    next_word_log_probs_1 = logsoftmax(next_word_logits_1)\n",
    "    \n",
    "    output_2 = model(input_ids = model_input_2[0],attention_mask = model_input_2[1] ) \n",
    "    #next_word_logits_2 = torch.stack([torch.index_select(seq_logits, 0, torch.tensor([i+1])) for i, seq_logits in enumerate(output_2.logits)])\n",
    "    next_word_logits_2 = output_2.logits[:, -1, :]\n",
    "    next_word_log_probs_2 = logsoftmax(next_word_logits_2)    \n",
    "\n",
    "    return next_word_log_probs_1, next_word_log_probs_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bc32b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = 5\n",
    "initialize_from_vocab = True\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2660928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BiasDataset(\"../data/occupation.csv\", n_tokens = 5)\n",
    "dataloader = dataset.get_dataloader(batch_size=64)\n",
    "\n",
    "#dataloader = DataLoader(dataset = dataset, batch_size = 4,  shuffle = False, num_workers=1, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c675faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "#freezing all the layers, and only keeping the tokens one active\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)\n",
    "    \n",
    "s_wte = SoftEmbedding(model.get_input_embeddings(), \n",
    "                      n_tokens=n_tokens, \n",
    "                      initialize_from_vocab=False)\n",
    "model.set_input_embeddings(s_wte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8a23314",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-2)\n",
    "logsoftmax = nn.LogSoftmax(dim = 0)\n",
    "kl_loss = nn.KLDivLoss(reduction=\"batchmean\", log_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f249f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55bae8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at batch : 0 is 0.34565556049346924\n",
      "loss at batch : 1 is 0.4419367015361786\n",
      "loss at batch : 2 is 1.2745273725300876e-09\n",
      "loss at batch : 3 is 0.06149400770664215\n",
      "loss at batch : 4 is 0.09740849584341049\n",
      "loss at batch : 5 is 0.8722692728042603\n",
      "loss at batch : 6 is 0.00013242590648587793\n",
      "loss at batch : 7 is 0.000687938358169049\n",
      "loss at batch : 8 is 1.5783867638674565e-05\n",
      "loss at batch : 9 is 0.0013247253373265266\n",
      "loss at batch : 10 is 0.0010080602951347828\n",
      "loss at batch : 11 is 0.02488534152507782\n",
      "loss at batch : 12 is 0.0008782527293078601\n",
      "loss at batch : 13 is 0.00010278807167196646\n",
      "loss at batch : 14 is 0.0012342375703155994\n",
      "loss at batch : 15 is 0.00044398586032912135\n",
      "loss at batch : 16 is 0.0027710909489542246\n",
      "loss at batch : 17 is 0.03998435288667679\n",
      "loss at batch : 18 is 0.0006562793860211968\n",
      "loss at batch : 19 is 0.0006301577086560428\n",
      "loss at batch : 20 is 0.0005002592806704342\n",
      "loss at batch : 21 is 0.0015243764501065016\n",
      "loss at batch : 22 is 0.0008560825372114778\n",
      "loss at batch : 23 is 0.00501652667298913\n",
      "loss at batch : 24 is 0.0005467738956212997\n",
      "loss at batch : 25 is 0.00426855031400919\n",
      "loss at batch : 26 is 0.0007877832977101207\n",
      "loss at batch : 27 is 0.016117727383971214\n",
      "loss at batch : 28 is 0.011830526404082775\n",
      "loss at batch : 29 is 0.0042680236510932446\n",
      "loss at batch : 30 is 0.0033733986783772707\n",
      "loss at batch : 31 is 0.004916531965136528\n",
      "loss at batch : 32 is 0.01064658910036087\n",
      "loss at batch : 33 is 0.0070741851814091206\n",
      "loss at batch : 34 is 0.01402202993631363\n",
      "loss at batch : 35 is 0.07366297394037247\n",
      "loss at batch : 36 is 0.003527393564581871\n",
      "loss at batch : 37 is 0.0016296441899612546\n",
      "loss at batch : 38 is 9.304470586357638e-05\n",
      "loss at batch : 39 is 0.004840047564357519\n",
      "loss at batch : 40 is 0.005474065896123648\n",
      "loss at batch : 41 is 0.008077718317508698\n",
      "loss at batch : 42 is 0.0030724790412932634\n",
      "loss at batch : 43 is 0.052237533032894135\n",
      "loss at batch : 44 is 0.0008300116169266403\n",
      "loss at batch : 45 is 0.0037492052651941776\n",
      "loss at batch : 46 is 0.004463044926524162\n",
      "loss at batch : 47 is 0.0272739976644516\n",
      "loss at batch : 48 is 0.0024146803189069033\n",
      "loss at batch : 49 is 0.0004306962073314935\n",
      "loss at batch : 50 is 0.0009622862562537193\n",
      "loss at batch : 51 is 0.0018494988325983286\n",
      "loss at batch : 52 is 0.0017148659098893404\n",
      "loss at batch : 53 is 0.011528415605425835\n",
      "loss at batch : 54 is 0.004072203300893307\n",
      "loss at batch : 55 is 0.0005957877147011459\n",
      "loss at batch : 56 is 0.007650633342564106\n",
      "loss at batch : 57 is 0.002919491846114397\n",
      "loss at batch : 58 is 0.002491262974217534\n",
      "loss at batch : 59 is 0.008913989178836346\n",
      "loss at batch : 60 is 0.04691130667924881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0371)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = 0\n",
    "curr_loss = []\n",
    "for iter, data in enumerate(dataloader):\n",
    "    group_1, group_2 = data[0], data[1]\n",
    "    with torch.no_grad():\n",
    "        log_probs1, log_probs2 = get_next_word_log_probabilities(data[0], data[1], model, logsoftmax)\n",
    "        loss = torch.mean(torch.stack([kl_loss(log_probs1[i], log_probs2[i]) for i in range(len(log_probs1))]))\n",
    "        curr_loss.append(loss)\n",
    "    print(f'loss at batch : {iter} is {loss.item()}')\n",
    "\n",
    "torch.mean(torch.stack(curr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5593708f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0\n",
      "loss at batch : 0 is 0.34565556049346924\n",
      "loss at batch : 1 is 0.0062172734178602695\n",
      "loss at batch : 2 is 0.0011024556588381529\n",
      "loss at batch : 3 is 0.0010622439440339804\n",
      "loss at batch : 4 is 0.003473471151664853\n",
      "loss at batch : 5 is 0.012821296229958534\n",
      "loss at batch : 6 is 0.000862563494592905\n",
      "loss at batch : 7 is 0.001400969922542572\n",
      "loss at batch : 8 is 0.0009375842055305839\n",
      "loss at batch : 9 is 2.2108559278422035e-05\n",
      "loss at batch : 10 is 3.261709935031831e-05\n",
      "loss at batch : 11 is 0.00013592277537100017\n",
      "loss at batch : 12 is 0.000782068760599941\n",
      "loss at batch : 13 is 0.0002840063825715333\n",
      "loss at batch : 14 is 0.00011298262688796967\n",
      "loss at batch : 15 is 7.823648047633469e-06\n",
      "loss at batch : 16 is 8.162546691892203e-06\n",
      "loss at batch : 17 is 0.00017380653298459947\n",
      "loss at batch : 18 is 0.005470344331115484\n",
      "loss at batch : 19 is 0.004626250825822353\n",
      "loss at batch : 20 is 0.0010765298502519727\n",
      "loss at batch : 21 is 6.0259271776885726e-06\n",
      "loss at batch : 22 is 1.16465689643519e-05\n",
      "loss at batch : 23 is 0.002129551023244858\n",
      "loss at batch : 24 is 0.0009509045630693436\n",
      "loss at batch : 25 is 0.00017756054876372218\n",
      "loss at batch : 26 is 0.0009869539644569159\n",
      "loss at batch : 27 is 6.51570298941806e-05\n",
      "loss at batch : 28 is 0.0009556959266774356\n",
      "loss at batch : 29 is 4.0513172280043364e-05\n",
      "loss at batch : 30 is 0.0006139234756119549\n",
      "loss at batch : 31 is 0.00025471195112913847\n",
      "loss at batch : 32 is 0.0005704152281396091\n",
      "loss at batch : 33 is 0.00037485730717889965\n",
      "loss at batch : 34 is 0.0005473867058753967\n",
      "loss at batch : 35 is 0.0002007616712944582\n",
      "loss at batch : 36 is 0.018609700724482536\n",
      "loss at batch : 37 is 0.0011709743412211537\n",
      "loss at batch : 38 is 0.00013242612476460636\n",
      "loss at batch : 39 is 1.3103775017953012e-05\n",
      "loss at batch : 40 is 0.00017176763503812253\n",
      "loss at batch : 41 is 6.973301788093522e-05\n",
      "loss at batch : 42 is 0.006868976168334484\n",
      "loss at batch : 43 is 0.0034154464956372976\n",
      "loss at batch : 44 is 0.0004966038395650685\n",
      "loss at batch : 45 is 4.388972229207866e-06\n",
      "loss at batch : 46 is 0.00047369342064484954\n",
      "loss at batch : 47 is 5.4626008932245895e-05\n",
      "loss at batch : 48 is 0.0005732065183110535\n",
      "loss at batch : 49 is 0.00014812216977588832\n",
      "loss at batch : 50 is 0.0005391668528318405\n",
      "loss at batch : 51 is 9.743416012497619e-05\n",
      "loss at batch : 52 is 6.932332325959578e-05\n",
      "loss at batch : 53 is 0.00028663137345574796\n",
      "loss at batch : 54 is 0.00022857461590319872\n",
      "loss at batch : 55 is 7.354112312896177e-05\n",
      "loss at batch : 56 is 5.238589801592752e-05\n",
      "loss at batch : 57 is 4.0887403883971274e-05\n",
      "loss at batch : 58 is 4.068644193466753e-05\n",
      "loss at batch : 59 is 3.657016350189224e-05\n",
      "loss at batch : 60 is 0.00011589500354602933\n"
     ]
    }
   ],
   "source": [
    "epoch_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    print(f'epoch : {epoch}')\n",
    "    loss = 0\n",
    "    curr_loss = []\n",
    "    for iter, data in enumerate(dataloader):\n",
    "        group_1, group_2 = data[0], data[1]\n",
    "        \n",
    "        log_probs1, log_probs2 = get_next_word_log_probabilities(data[0], data[1], model, logsoftmax)\n",
    "        #import pdb; pdb.set_trace()\n",
    "        #loss = kl_loss(log_probs1, log_probs2)\n",
    "        loss = torch.mean(torch.stack([kl_loss(log_probs1[i], log_probs2[i]) for i in range(len(log_probs1))]))\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'loss at batch : {iter} is {loss.item()}')\n",
    "\n",
    "    epoch_loss.append(curr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44af37e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc448dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get the training loss before and after training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efa6c3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0\n",
      "loss at batch : 0 is 0.001299596973694861\n",
      "loss at batch : 1 is 0.0010230415500700474\n",
      "loss at batch : 2 is 0.00041725795017555356\n",
      "loss at batch : 3 is 0.000376284820958972\n",
      "loss at batch : 4 is 0.00036795789492316544\n",
      "loss at batch : 5 is 0.0037470136303454638\n",
      "loss at batch : 6 is 3.571749766706489e-05\n",
      "loss at batch : 7 is 2.6185654860455543e-05\n",
      "loss at batch : 8 is 1.8900866052717902e-05\n",
      "loss at batch : 9 is 1.4819745956629049e-05\n",
      "loss at batch : 10 is 1.2007320037810132e-05\n",
      "loss at batch : 11 is 2.8479964385041967e-05\n",
      "loss at batch : 12 is 0.00028187481802888215\n",
      "loss at batch : 13 is 0.00013327850319910794\n",
      "loss at batch : 14 is 0.00011007210559910163\n",
      "loss at batch : 15 is 6.803842552471906e-05\n",
      "loss at batch : 16 is 6.454249523812905e-05\n",
      "loss at batch : 17 is 0.00023160297132562846\n",
      "loss at batch : 18 is 0.00016904796939343214\n",
      "loss at batch : 19 is 7.315910625038669e-05\n",
      "loss at batch : 20 is 0.00011762839858420193\n",
      "loss at batch : 21 is 9.105166827794164e-05\n",
      "loss at batch : 22 is 8.73649405548349e-05\n",
      "loss at batch : 23 is 7.985336560523137e-05\n",
      "loss at batch : 24 is 0.0009009854984469712\n",
      "loss at batch : 25 is 0.00037484406493604183\n",
      "loss at batch : 26 is 0.00017077564552892\n",
      "loss at batch : 27 is 0.00012285042612347752\n",
      "loss at batch : 28 is 8.331042772624642e-05\n",
      "loss at batch : 29 is 0.000532076635863632\n",
      "loss at batch : 30 is 0.0011514413636177778\n",
      "loss at batch : 31 is 0.00047970705782063305\n",
      "loss at batch : 32 is 0.00045664695790037513\n",
      "loss at batch : 33 is 0.000538581982254982\n",
      "loss at batch : 34 is 0.0004937252961099148\n",
      "loss at batch : 35 is 0.0007833887357264757\n",
      "loss at batch : 36 is 0.0003341306874062866\n",
      "loss at batch : 37 is 0.00019948184490203857\n",
      "loss at batch : 38 is 0.0001333390682702884\n",
      "loss at batch : 39 is 9.404059528606012e-05\n",
      "loss at batch : 40 is 8.487933519063517e-05\n",
      "loss at batch : 41 is 0.00016435841098427773\n",
      "loss at batch : 42 is 0.0013117159251123667\n",
      "loss at batch : 43 is 0.00015679094940423965\n",
      "loss at batch : 44 is 0.00016073507140390575\n",
      "loss at batch : 45 is 5.486490408657119e-05\n",
      "loss at batch : 46 is 5.37768573849462e-05\n",
      "loss at batch : 47 is 0.00022495115990750492\n",
      "loss at batch : 48 is 0.00021524325711652637\n",
      "loss at batch : 49 is 9.467657946515828e-05\n",
      "loss at batch : 50 is 5.473150667967275e-05\n",
      "loss at batch : 51 is 5.266244988888502e-05\n",
      "loss at batch : 52 is 6.0749382100766525e-05\n",
      "loss at batch : 53 is 0.00014242502220440656\n",
      "loss at batch : 54 is 0.00011703002383001149\n",
      "loss at batch : 55 is 7.266816101036966e-05\n",
      "loss at batch : 56 is 3.990212644566782e-05\n",
      "loss at batch : 57 is 3.181407373631373e-05\n",
      "loss at batch : 58 is 3.871039371006191e-05\n",
      "loss at batch : 59 is 3.733882840606384e-05\n",
      "loss at batch : 60 is 0.00011588381312321872\n"
     ]
    }
   ],
   "source": [
    "print(f'epoch : {epoch}')\n",
    "loss = 0\n",
    "curr_loss = []\n",
    "for iter, data in enumerate(dataloader):\n",
    "    group_1, group_2 = data[0], data[1]\n",
    "    with torch.no_grad():\n",
    "        log_probs1, log_probs2 = get_next_word_log_probabilities(data[0], data[1], model, logsoftmax)\n",
    "        loss = torch.mean(torch.stack([kl_loss(log_probs1[i], log_probs2[i]) for i in range(len(log_probs1))]))\n",
    "        curr_loss.append(loss)\n",
    "    print(f'loss at batch : {iter} is {loss.item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "838ab430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0003)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.stack(curr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc65cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "this is the KL loss before training with the hard prompt- tensor(0.0054)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
